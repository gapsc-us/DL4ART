{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14daac6",
   "metadata": {
    "id": "tutorial-migration"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9533fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JJMF 20230201 \n",
    "# We changed the folder paths\n",
    "# We tried to save the loss and val_loss results for every iteration, both the complete history and the best values\n",
    "# We created version create_datasetV9 that includes the option of a seed to generate the same data set for all the 10\n",
    "# trainings, then used it here\n",
    "seedCreateDatasetV9 = 41 #+ umber of trained, added later in the code\n",
    "# This way we provide 10 different datasets to the methods, but the same 10 \n",
    "# We included the value of batch size\n",
    "# We give the best model to the test final part\n",
    "# LR is used when calling to optimization\n",
    "\n",
    "# 20230305 JJMF We forgot to set the LR to a fixed rate... we rerun it with LR = 1.0e-3 y dropout a 0 and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b450d6d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mediterranean-group",
    "outputId": "a1d1d7b0-35ff-4388-b9d2-6d7327b2e018",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 13:44:33.721930: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2576727/2310047592.py:43: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 13:44:34.241642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46713 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:1a:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import os\n",
    "sys.path.append(os.getcwd()+'/../Utils')\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "import scipy\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Functions and Regression models:\n",
    "from RegressionModelsV5gh import *\n",
    "from preprocess_countV14gh import *\n",
    "\n",
    "import time\n",
    "'''\n",
    "if not tf.test.is_gpu_available():\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "'''\n",
    "\n",
    "\n",
    "#JJMF 20230324 code is available for NVIDIA GPU \n",
    "CUDA_VISIBLE_DEVICES=0 #./cuda_executable\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "\n",
    "# Para limpiar la memoria de la GPU entre iteraciones\n",
    "import tensorflow.keras.backend as K\n",
    "import gc\n",
    "import nvidia_smi\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "from datetime import datetime\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "#JJMF 20230324 In our setup we have a list of X-ray of canvases and here we indicate which one\n",
    "#to use in each subdataset. For each image we have about 7-10 samples labeled. Labeling consist \n",
    "#of polygons around each thread\n",
    "trainImages = [0,2,3,6,5,5,8,8,10,10,12,12,13,13,15,16,16,17,17,18,18,20,20,21,21,22,22,\n",
    "               23,24,24,25,26,27,28,28,29,30,31,32,33,34,35,36]\n",
    "validImages = [4, 4, 9, 9, 1, 1, 14]\n",
    "testImages = [11, 7, 19]\n",
    "\n",
    "numberRotations = 1 ##JJMF 20230324 indicate how many rotations are included in Data Augmentation (DA)\n",
    "Elastic = False #JJMF 20230324 if elastic deformations is used in Data Augmentation (DA)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "##JJMF 20230324 We define our function loss\n",
    "def normalizedRMSE(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)/y_true, axis=-1)) \n",
    "    \n",
    "def normalizedMSE(y_true, y_pred):\n",
    "        return (K.mean(K.square(y_pred - y_true)/y_true, axis=-1)) \n",
    "\n",
    "def normalizedMAE(y_true, y_pred):\n",
    "        return K.mean(K.abs(y_pred - y_true)/y_true, axis=-1) \n",
    "    \n",
    "##JJMF 20230324 indicate paths\n",
    "folderIPYNB =  os.path.abspath(os.getcwd())\n",
    "folderCrops = folderIPYNB + '/../Utils/cropsVentanaREG/' #JJMF 20230324 paths where original pre-processed images are                                                       \n",
    "### Folder with npz files path:\n",
    "folderNPZ = folderIPYNB + '/../Utils/LabeledNEv7_npz/' #JJMF 20230324 binary images with location of crossing points\n",
    "folderSave = folderIPYNB+ '/Trained_models/VGGEqNMAEBS32/' ##JJMF 20230324 folder to save learnt weights of the model\n",
    "\n",
    "#path = os.path.dirname(saveFileName)\n",
    "if not os.path.exists(folderSave):\n",
    "    os.makedirs(folderSave)\n",
    "    \n",
    "#Model\n",
    "numberOfFilters = 8\n",
    "pbDroput = 0.05\n",
    "lossFunc = 'mse'\n",
    "nameModel2Save ='RegressionVGG'  ##JJMF 20230324 name of the model used, see RegressionModelsV6gh.py \n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience = 10) #patience=65)    \n",
    "EPOCHS = 100 #450\n",
    "LR = 1.0e-3\n",
    "seedCreateDatasetV9 = 41\n",
    "numberOfTrainings = 2 #10 #JJMF 20230324 Important, the number of different learning of the model\n",
    "         #with different DA and initialization of the weights. If you reuse the code for other model\n",
    "         #you will see that the DA generated for every training is the same, by using a different seed \n",
    "         #given by seedCreateDatasetV9+l with l the number of the train.\n",
    "BatchSize = 32 \n",
    "\n",
    "def lr_time_based_decay(epoch, learning_rate):\n",
    "    return LR #* 1 / (1 + epoch/10) #JJMF 20230324  fixed rate\n",
    "sch = LearningRateScheduler(lr_time_based_decay, verbose=1)\n",
    "modelFunc = 'regVGG(input_shape = (200,200,1), numFilters = numberOfFilters, dropout = pbDroput)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f4236",
   "metadata": {
    "id": "under-hopkins"
   },
   "source": [
    "# Preparing the Data (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f13b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df24e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/../Utils/LabeledNEv7_npz/\n",
      "Number of read patches:240\n",
      "Number of read patches generated per image:40\n",
      "Images with some labeled crops: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36] \n",
      "\n",
      "Number of labels per image: [7, 7, 8, 10, 8, 7, 11, 4, 10, 6, 11, 8, 10, 5, 10, 7, 12, 4, 7, 6, 10, 10, 5, 5, 4, 5, 5, 5, 4, 3, 3, 2, 4, 4, 3, 5, 5]\n",
      "Labeled Crops for every Images:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Crop0012',\n",
       "  'Crop0013',\n",
       "  'Crop0015',\n",
       "  'Crop0022',\n",
       "  'Crop0028',\n",
       "  'Crop0031',\n",
       "  'Crop0036'],\n",
       " ['Crop0041',\n",
       "  'Crop0058',\n",
       "  'Crop0062',\n",
       "  'Crop0065',\n",
       "  'Crop0073',\n",
       "  'Crop0074',\n",
       "  'Crop0080'],\n",
       " ['Crop0081',\n",
       "  'Crop0084',\n",
       "  'Crop0087',\n",
       "  'Crop0088',\n",
       "  'Crop0097',\n",
       "  'Crop0101',\n",
       "  'Crop0102',\n",
       "  'Crop0104'],\n",
       " ['Crop0123',\n",
       "  'Crop0125',\n",
       "  'Crop0126',\n",
       "  'Crop0132',\n",
       "  'Crop0139',\n",
       "  'Crop0145',\n",
       "  'Crop0147',\n",
       "  'Crop0152',\n",
       "  'Crop0156',\n",
       "  'Crop0159'],\n",
       " ['Crop0165',\n",
       "  'Crop0170',\n",
       "  'Crop0175',\n",
       "  'Crop0178',\n",
       "  'Crop0184',\n",
       "  'Crop0187',\n",
       "  'Crop0192',\n",
       "  'Crop0195'],\n",
       " ['Crop0201',\n",
       "  'Crop0212',\n",
       "  'Crop0219',\n",
       "  'Crop0221',\n",
       "  'Crop0223',\n",
       "  'Crop0226',\n",
       "  'Crop0236'],\n",
       " ['Crop0243',\n",
       "  'Crop0244',\n",
       "  'Crop0245',\n",
       "  'Crop0247',\n",
       "  'Crop0249',\n",
       "  'Crop0250',\n",
       "  'Crop0253',\n",
       "  'Crop0272',\n",
       "  'Crop0274',\n",
       "  'Crop0275',\n",
       "  'Crop0277'],\n",
       " ['Crop0299', 'Crop0302', 'Crop0307', 'Crop0315'],\n",
       " ['Crop0321',\n",
       "  'Crop0322',\n",
       "  'Crop0323',\n",
       "  'Crop0324',\n",
       "  'Crop0325',\n",
       "  'Crop0326',\n",
       "  'Crop0327',\n",
       "  'Crop0328',\n",
       "  'Crop0329',\n",
       "  'Crop0330'],\n",
       " ['Crop0361', 'Crop0374', 'Crop0376', 'Crop0378', 'Crop0391', 'Crop0400'],\n",
       " ['Crop0401',\n",
       "  'Crop0402',\n",
       "  'Crop0408',\n",
       "  'Crop0410',\n",
       "  'Crop0417',\n",
       "  'Crop0419',\n",
       "  'Crop0423',\n",
       "  'Crop0428',\n",
       "  'Crop0433',\n",
       "  'Crop0435',\n",
       "  'Crop0438'],\n",
       " ['Crop0441',\n",
       "  'Crop0447',\n",
       "  'Crop0453',\n",
       "  'Crop0456',\n",
       "  'Crop0465',\n",
       "  'Crop0477',\n",
       "  'Crop0479',\n",
       "  'Crop0480'],\n",
       " ['Crop0483',\n",
       "  'Crop0490',\n",
       "  'Crop0491',\n",
       "  'Crop0492',\n",
       "  'Crop0494',\n",
       "  'Crop0506',\n",
       "  'Crop0508',\n",
       "  'Crop0510',\n",
       "  'Crop0514',\n",
       "  'Crop0518'],\n",
       " ['Crop0527', 'Crop0542', 'Crop0544', 'Crop0551', 'Crop0556'],\n",
       " ['Crop0561',\n",
       "  'Crop0562',\n",
       "  'Crop0563',\n",
       "  'Crop0567',\n",
       "  'Crop0573',\n",
       "  'Crop0576',\n",
       "  'Crop0582',\n",
       "  'Crop0585',\n",
       "  'Crop0595',\n",
       "  'Crop0597'],\n",
       " ['Crop0609',\n",
       "  'Crop0613',\n",
       "  'Crop0617',\n",
       "  'Crop0622',\n",
       "  'Crop0628',\n",
       "  'Crop0630',\n",
       "  'Crop0636'],\n",
       " ['Crop0641',\n",
       "  'Crop0642',\n",
       "  'Crop0643',\n",
       "  'Crop0644',\n",
       "  'Crop0645',\n",
       "  'Crop0646',\n",
       "  'Crop0647',\n",
       "  'Crop0648',\n",
       "  'Crop0649',\n",
       "  'Crop0652',\n",
       "  'Crop0655',\n",
       "  'Crop0679'],\n",
       " ['Crop0682', 'Crop0686', 'Crop0702', 'Crop0717'],\n",
       " ['Crop0722',\n",
       "  'Crop0730',\n",
       "  'Crop0738',\n",
       "  'Crop0746',\n",
       "  'Crop0751',\n",
       "  'Crop0756',\n",
       "  'Crop0760'],\n",
       " ['Crop0762', 'Crop0766', 'Crop0775', 'Crop0778', 'Crop0780', 'Crop0795'],\n",
       " ['Crop0801',\n",
       "  'Crop0804',\n",
       "  'Crop0805',\n",
       "  'Crop0810',\n",
       "  'Crop0813',\n",
       "  'Crop0818',\n",
       "  'Crop0821',\n",
       "  'Crop0826',\n",
       "  'Crop0837',\n",
       "  'Crop0838'],\n",
       " ['Crop0846',\n",
       "  'Crop0857',\n",
       "  'Crop0858',\n",
       "  'Crop0860',\n",
       "  'Crop0864',\n",
       "  'Crop0865',\n",
       "  'Crop0870',\n",
       "  'Crop0874',\n",
       "  'Crop0876',\n",
       "  'Crop0877'],\n",
       " ['Crop0881', 'Crop0887', 'Crop0890', 'Crop0902', 'Crop0918'],\n",
       " ['Crop0929', 'Crop0933', 'Crop0938', 'Crop0947', 'Crop0955'],\n",
       " ['Crop0963', 'Crop0967', 'Crop0973', 'Crop0987'],\n",
       " ['Crop1003', 'Crop1021', 'Crop1024', 'Crop1031', 'Crop1035'],\n",
       " ['Crop1044', 'Crop1050', 'Crop1057', 'Crop1065', 'Crop1071'],\n",
       " ['Crop1092', 'Crop1098', 'Crop1102', 'Crop1106', 'Crop1118'],\n",
       " ['Crop1127', 'Crop1138', 'Crop1142', 'Crop1155'],\n",
       " ['Crop1165', 'Crop1175', 'Crop1197'],\n",
       " ['Crop1217', 'Crop1231', 'Crop1235'],\n",
       " ['Crop1248', 'Crop1268'],\n",
       " ['Crop1284', 'Crop1289', 'Crop1294', 'Crop1299'],\n",
       " ['Crop1342', 'Crop1347', 'Crop1353', 'Crop1355'],\n",
       " ['Crop1364', 'Crop1368', 'Crop1396'],\n",
       " ['Crop1406', 'Crop1412', 'Crop1427', 'Crop1435', 'Crop1439'],\n",
       " ['Crop1444', 'Crop1450', 'Crop1454', 'Crop1467', 'Crop1475']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#JJMF 20230324 In forlder folderNPZ we have .npz files with labels named CropsXXXX.npz. In folderNPZ we will have a list of files, one per laTherefore the m x 40 points + n with 0<=n<=40 points to the \n",
    "#labels for m-th canvas, XXXX. But as we generate just some labels out of 40 the \n",
    "# samples taken from each canvas, we do not have a complete continuous list of images. For example, if we labeled sample \n",
    "# 0, 3, 7, 34, 83, 94, 97, 102, 104, in the folder you will find files Crops0000.npz, Crops0003.npz, Crops0034.npz, Crops0083.npz,\n",
    "# and so on, corresponding to canvases number 0 and 3. The corresponding input images will be also used, taken from\n",
    "# folderCrops \n",
    "#\n",
    "#In this cell we generate the indexes to the samples (labels and images), and generate three list of indexs for training,\n",
    "#validation and test. Images are 1.5 cm side and 200 px per cm, therefore 300 pixels side square images.\n",
    "\n",
    "print(folderNPZ)\n",
    "# Alphabetically ordered list:\n",
    "os.chdir(folderNPZ)\n",
    "content = os.listdir(folderNPZ)\n",
    "content.sort()\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "\n",
    "# Remove .DS_Store file if exists:\n",
    "if content[0] == '.DS_Store':\n",
    "    content.pop(0)\n",
    "    \n",
    "# We read those crops that have been labeled (Crops name are duplicated since there are two npz files for each crop)\n",
    "listCropsLabeled = []\n",
    "listCropsLabeledNumber = []\n",
    "\n",
    "for crop in content:\n",
    "    listCropsLabeled.append(crop[:-6])  #JJMF we remove _H.npz or _V.npz to get the crop name\n",
    "    listCropsLabeledNumber.append(int(crop[4:-6]))\n",
    "\n",
    "    \n",
    "listCropsLabeled = list(set(listCropsLabeled)) #JJMF We get the unique names, i.e. remove repeated names\n",
    "listCropsLabeled.sort()\n",
    "listCropsLabeledNumber = list(set(listCropsLabeledNumber))\n",
    "listCropsLabeledNumber.sort()\n",
    "\n",
    "\n",
    "# Number of different labeled crops at the list\n",
    "numberOfLabels = len(listCropsLabeled)\n",
    "print('Number of read patches:{}'.format(numberOfLabels )) \n",
    "\n",
    "# Number of patches obtained from each image\n",
    "numberPatchesPerImage = 40  # Fixed on notebook \"newCropImagesOO\"\n",
    "print('Number of read patches generated per image:{}'.format(numberPatchesPerImage)) \n",
    "    \n",
    "idxOfImg = (np.asarray(listCropsLabeledNumber)-1) // numberPatchesPerImage #//40\n",
    "\n",
    "#JJMF 20200306 This is important to know how many labeled patches of every image do we have.\n",
    "vl,labelsPerImages = np.unique(idxOfImg, return_counts=True)\n",
    "\n",
    "#JJMF 20200303 \n",
    "#We need to know what label belong to what image\n",
    "\n",
    "indexCanvas = []\n",
    "auxCanvas = []\n",
    "prev = idxOfImg[0]\n",
    "canvasMapNumber = []\n",
    "labelsPerImage =[]\n",
    "for k1 in range(numberOfLabels):\n",
    "    \n",
    "\n",
    "    if idxOfImg[k1] != prev:\n",
    "        indexCanvas.append(auxCanvas)\n",
    "        labelsPerImage.append(len(auxCanvas))\n",
    "        for k2 in range(idxOfImg[k1]-prev-1):\n",
    "                indexCanvas.append([])\n",
    "                labelsPerImage.append(0)\n",
    "        auxCanvas = []\n",
    "        canvasMapNumber.append(prev)\n",
    "        prev = idxOfImg[k1]\n",
    "    \n",
    "    auxCanvas.append(listCropsLabeled[k1])\n",
    "    \n",
    "    \n",
    "#JJMF 20220307    \n",
    "#At exit of for we need to save the labels for the last image             \n",
    "indexCanvas.append(auxCanvas)\n",
    "labelsPerImage.append(len(auxCanvas))\n",
    "canvasMapNumber.append(prev)\n",
    "    \n",
    "indexCanvas=list(indexCanvas)\n",
    "\n",
    "\n",
    "#JJMF 20230324 generate indexes to the samples for every data subset\n",
    "#Complete list of images\n",
    "idxImgs = np.asarray(trainImages+validImages+testImages)\n",
    "#len(idxImgs)\n",
    "\n",
    "trainLen = len(trainImages)\n",
    "validLen = len(validImages)\n",
    "testLen = len(testImages)\n",
    "\n",
    "print('Images with some labeled crops:',canvasMapNumber,'\\n')\n",
    "print('Number of labels per image:', labelsPerImage)\n",
    "\n",
    "print('Labeled Crops for every Images:')\n",
    "indexCanvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a2e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cef9425b",
   "metadata": {},
   "source": [
    "# ANN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75c12291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 13:44:34.487794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46713 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:1a:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = eval(modelFunc)\n",
    "model.compile(optimizer = Adam(learning_rate = LR), loss = lossFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e5953c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 200, 200, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 200, 200, 8)  80          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 200, 200, 8)  208         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 200, 200, 8)  400         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 200, 200, 8)  32         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 200, 200, 8)  32         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 200, 200, 8)  32         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 200, 200, 8)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 200, 200, 8)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 200, 200, 8)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 200, 200, 24  0           ['activation[0][0]',             \n",
      "                                )                                 'activation_1[0][0]',           \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 200, 200, 8)  1736        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 200, 200, 8)  4808        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 200, 200, 8)  9416        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 200, 200, 8)  32         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 200, 200, 8)  32         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 200, 200, 8)  32         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 200, 200, 8)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 200, 200, 8)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 200, 200, 8)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 200, 200, 24  0           ['activation_3[0][0]',           \n",
      "                                )                                 'activation_4[0][0]',           \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 100, 100, 24  0           ['concatenate_1[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 100, 100, 24  0           ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 100, 100, 16  3472        ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 100, 100, 16  9616        ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 100, 100, 16  18832       ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 100, 100, 16  64         ['conv2d_6[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 100, 100, 16  64         ['conv2d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 100, 100, 16  64         ['conv2d_8[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 100, 100, 16  0           ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 100, 100, 16  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 100, 100, 16  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 100, 100, 48  0           ['activation_6[0][0]',           \n",
      "                                )                                 'activation_7[0][0]',           \n",
      "                                                                  'activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 100, 100, 16  6928        ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 100, 100, 16  19216       ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 100, 100, 16  37648       ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 100, 100, 16  64         ['conv2d_9[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 100, 100, 16  64         ['conv2d_10[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 100, 100, 16  64         ['conv2d_11[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 100, 100, 16  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 100, 100, 16  0           ['batch_normalization_10[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 100, 100, 16  0           ['batch_normalization_11[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 100, 100, 48  0           ['activation_9[0][0]',           \n",
      "                                )                                 'activation_10[0][0]',          \n",
      "                                                                  'activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 50, 50, 48)  0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 50, 48)   0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 50, 50, 32)   13856       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 50, 50, 32)   38432       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 50, 50, 32)   75296       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 50, 50, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 50, 50, 32)  128         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 50, 50, 32)  128         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 50, 50, 32)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 50, 50, 32)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 50, 50, 32)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 50, 50, 96)   0           ['activation_12[0][0]',          \n",
      "                                                                  'activation_13[0][0]',          \n",
      "                                                                  'activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 50, 50, 32)   27680       ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 50, 50, 32)   76832       ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 50, 50, 32)   150560      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 50, 50, 32)  128         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 50, 50, 32)  128         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 50, 50, 32)  128         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 50, 50, 32)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 50, 50, 32)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 50, 50, 32)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 50, 50, 96)   0           ['activation_15[0][0]',          \n",
      "                                                                  'activation_16[0][0]',          \n",
      "                                                                  'activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 50, 50, 32)   27680       ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 50, 50, 32)   76832       ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 50, 50, 32)   150560      ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 50, 50, 32)  128         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 50, 50, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 50, 50, 32)  128         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 50, 50, 32)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 50, 50, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 50, 50, 32)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 50, 50, 96)   0           ['activation_18[0][0]',          \n",
      "                                                                  'activation_19[0][0]',          \n",
      "                                                                  'activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 25, 25, 96)  0           ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 25, 25, 96)   0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 25, 25, 64)   55360       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 25, 25, 64)   153664      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 25, 25, 64)   301120      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 25, 25, 64)  256         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 25, 25, 64)  256         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 25, 25, 64)  256         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 25, 25, 192)  0           ['activation_21[0][0]',          \n",
      "                                                                  'activation_22[0][0]',          \n",
      "                                                                  'activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 25, 25, 64)   110656      ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 25, 25, 64)   307264      ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 25, 25, 64)   602176      ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 25, 25, 64)  256         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 25, 25, 64)  256         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 25, 25, 64)  256         ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 25, 25, 192)  0           ['activation_24[0][0]',          \n",
      "                                                                  'activation_25[0][0]',          \n",
      "                                                                  'activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 25, 25, 64)   110656      ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 25, 25, 64)   307264      ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 25, 25, 64)   602176      ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 25, 25, 64)  256         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 25, 25, 64)  256         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 25, 25, 64)  256         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 25, 25, 192)  0           ['activation_27[0][0]',          \n",
      "                                                                  'activation_28[0][0]',          \n",
      "                                                                  'activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 12, 12, 192)  0          ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 12, 12, 192)  0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 12, 12, 64)   110656      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 12, 12, 64)   307264      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 12, 12, 64)   602176      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 12, 12, 64)  256         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 12, 12, 64)  256         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 12, 12, 64)  256         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 12, 12, 192)  0           ['activation_30[0][0]',          \n",
      "                                                                  'activation_31[0][0]',          \n",
      "                                                                  'activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 12, 12, 64)   110656      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 12, 12, 64)   307264      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 12, 12, 64)   602176      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 12, 12, 64)  256         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 12, 12, 64)  256         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 12, 12, 64)  256         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 12, 12, 192)  0           ['activation_33[0][0]',          \n",
      "                                                                  'activation_34[0][0]',          \n",
      "                                                                  'activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 12, 12, 64)   110656      ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 12, 12, 64)   307264      ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 12, 12, 64)   602176      ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 12, 12, 64)  256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 12, 12, 64)  256         ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 12, 12, 64)  256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 12, 12, 192)  0           ['activation_36[0][0]',          \n",
      "                                                                  'activation_37[0][0]',          \n",
      "                                                                  'activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 6, 6, 192)   0           ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 6, 6, 192)    0           ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 6912)         0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          3539456     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            513         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,169,673\n",
      "Trainable params: 10,166,505\n",
      "Non-trainable params: 3,168\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce4c47b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sixth-pizza",
    "outputId": "2970bf97-0e48-48b3-8553-2b52ff6b8a80",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " NUMBER OF TRAINING 1\n",
      "Creating dataset...\n",
      "Rotating 90º each 200x200 crop...\n",
      "Converting to 0-1 range...\n",
      "Generating counting labels... \n",
      "Execution time in generating samples:388.02500778000103\n",
      "Instancing new model...\n",
      "Training...\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 13:51:14.438567: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\n",
      "2023-03-24 13:51:16.220063: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-03-24 13:51:18.434006: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762/762 [==============================] - ETA: 0s - loss: 0.1179 - normalizedMAE: 0.1179\n",
      "Epoch 00001: val_loss improved from inf to 0.18332, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_0.h5\n",
      "762/762 [==============================] - 87s 102ms/step - loss: 0.1179 - normalizedMAE: 0.1179 - val_loss: 0.1833 - val_normalizedMAE: 0.1833 - lr: 0.0010\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0529 - normalizedMAE: 0.0529\n",
      "Epoch 00002: val_loss improved from 0.18332 to 0.03666, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_0.h5\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0529 - normalizedMAE: 0.0529 - val_loss: 0.0367 - val_normalizedMAE: 0.0367 - lr: 0.0010\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0431 - normalizedMAE: 0.0431\n",
      "Epoch 00003: val_loss improved from 0.03666 to 0.03319, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_0.h5\n",
      "762/762 [==============================] - 77s 100ms/step - loss: 0.0431 - normalizedMAE: 0.0431 - val_loss: 0.0332 - val_normalizedMAE: 0.0332 - lr: 0.0010\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.0375 - normalizedMAE: 0.0375\n",
      "Epoch 00004: val_loss did not improve from 0.03319\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0375 - normalizedMAE: 0.0375 - val_loss: 0.0347 - val_normalizedMAE: 0.0347 - lr: 0.0010\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0331 - normalizedMAE: 0.0331\n",
      "Epoch 00005: val_loss improved from 0.03319 to 0.02632, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_0.h5\n",
      "762/762 [==============================] - 77s 101ms/step - loss: 0.0331 - normalizedMAE: 0.0331 - val_loss: 0.0263 - val_normalizedMAE: 0.0263 - lr: 0.0010\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0310 - normalizedMAE: 0.0310\n",
      "Epoch 00006: val_loss did not improve from 0.02632\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0310 - normalizedMAE: 0.0310 - val_loss: 0.0415 - val_normalizedMAE: 0.0415 - lr: 0.0010\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0282 - normalizedMAE: 0.0282\n",
      "Epoch 00007: val_loss did not improve from 0.02632\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0282 - normalizedMAE: 0.0282 - val_loss: 0.0320 - val_normalizedMAE: 0.0320 - lr: 0.0010\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 8/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0267 - normalizedMAE: 0.0267\n",
      "Epoch 00008: val_loss improved from 0.02632 to 0.01593, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_0.h5\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0267 - normalizedMAE: 0.0267 - val_loss: 0.0159 - val_normalizedMAE: 0.0159 - lr: 0.0010\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 9/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0248 - normalizedMAE: 0.0248\n",
      "Epoch 00009: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0248 - normalizedMAE: 0.0248 - val_loss: 0.0180 - val_normalizedMAE: 0.0180 - lr: 0.0010\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 10/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0233 - normalizedMAE: 0.0233\n",
      "Epoch 00010: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0233 - normalizedMAE: 0.0233 - val_loss: 0.0366 - val_normalizedMAE: 0.0366 - lr: 0.0010\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 11/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.0220 - normalizedMAE: 0.0220\n",
      "Epoch 00011: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0220 - normalizedMAE: 0.0220 - val_loss: 0.0228 - val_normalizedMAE: 0.0228 - lr: 0.0010\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 12/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0217 - normalizedMAE: 0.0217\n",
      "Epoch 00012: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0217 - normalizedMAE: 0.0217 - val_loss: 0.0279 - val_normalizedMAE: 0.0279 - lr: 0.0010\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 13/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0194 - normalizedMAE: 0.0194\n",
      "Epoch 00013: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0194 - normalizedMAE: 0.0194 - val_loss: 0.0199 - val_normalizedMAE: 0.0199 - lr: 0.0010\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 14/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0189 - normalizedMAE: 0.0189\n",
      "Epoch 00014: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0189 - normalizedMAE: 0.0189 - val_loss: 0.0215 - val_normalizedMAE: 0.0215 - lr: 0.0010\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 15/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.0179 - normalizedMAE: 0.0179\n",
      "Epoch 00015: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0179 - normalizedMAE: 0.0179 - val_loss: 0.0193 - val_normalizedMAE: 0.0193 - lr: 0.0010\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 16/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0175 - normalizedMAE: 0.0175\n",
      "Epoch 00016: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0175 - normalizedMAE: 0.0175 - val_loss: 0.0253 - val_normalizedMAE: 0.0253 - lr: 0.0010\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 17/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0168 - normalizedMAE: 0.0168\n",
      "Epoch 00017: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0168 - normalizedMAE: 0.0168 - val_loss: 0.0204 - val_normalizedMAE: 0.0204 - lr: 0.0010\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 18/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0166 - normalizedMAE: 0.0166\n",
      "Epoch 00018: val_loss did not improve from 0.01593\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0166 - normalizedMAE: 0.0166 - val_loss: 0.0444 - val_normalizedMAE: 0.0444 - lr: 0.0010\n",
      "Cleaning GPU memory...\n",
      "Execution time in Training:1396.748306826048\n",
      "\n",
      " NUMBER OF TRAINING 2\n",
      "Creating dataset...\n",
      "Rotating 90º each 200x200 crop...\n",
      "Converting to 0-1 range...\n",
      "Generating counting labels... \n",
      "Execution time in generating samples:393.679132098041\n",
      "Instancing new model...\n",
      "Training...\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.1171 - normalizedMAE: 0.1171\n",
      "Epoch 00001: val_loss improved from inf to 0.03760, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_1.h5\n",
      "762/762 [==============================] - 80s 101ms/step - loss: 0.1170 - normalizedMAE: 0.1170 - val_loss: 0.0376 - val_normalizedMAE: 0.0376 - lr: 0.0010\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761/762 [============================>.] - ETA: 0s - loss: 0.0483 - normalizedMAE: 0.0483\n",
      "Epoch 00002: val_loss did not improve from 0.03760\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0483 - normalizedMAE: 0.0483 - val_loss: 0.0580 - val_normalizedMAE: 0.0580 - lr: 0.0010\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.0391 - normalizedMAE: 0.0391\n",
      "Epoch 00003: val_loss did not improve from 0.03760\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0391 - normalizedMAE: 0.0391 - val_loss: 0.0474 - val_normalizedMAE: 0.0474 - lr: 0.0010\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0356 - normalizedMAE: 0.0356\n",
      "Epoch 00004: val_loss improved from 0.03760 to 0.01703, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_1.h5\n",
      "762/762 [==============================] - 77s 100ms/step - loss: 0.0356 - normalizedMAE: 0.0356 - val_loss: 0.0170 - val_normalizedMAE: 0.0170 - lr: 0.0010\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0334 - normalizedMAE: 0.0334\n",
      "Epoch 00005: val_loss did not improve from 0.01703\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0334 - normalizedMAE: 0.0334 - val_loss: 0.0576 - val_normalizedMAE: 0.0576 - lr: 0.0010\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0303 - normalizedMAE: 0.0303\n",
      "Epoch 00006: val_loss did not improve from 0.01703\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0303 - normalizedMAE: 0.0303 - val_loss: 0.0192 - val_normalizedMAE: 0.0192 - lr: 0.0010\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0280 - normalizedMAE: 0.0280\n",
      "Epoch 00007: val_loss did not improve from 0.01703\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0280 - normalizedMAE: 0.0280 - val_loss: 0.1145 - val_normalizedMAE: 0.1145 - lr: 0.0010\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 8/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0270 - normalizedMAE: 0.0270\n",
      "Epoch 00008: val_loss did not improve from 0.01703\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0270 - normalizedMAE: 0.0270 - val_loss: 0.0224 - val_normalizedMAE: 0.0224 - lr: 0.0010\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 9/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.0259 - normalizedMAE: 0.0259\n",
      "Epoch 00009: val_loss did not improve from 0.01703\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0259 - normalizedMAE: 0.0259 - val_loss: 0.0437 - val_normalizedMAE: 0.0437 - lr: 0.0010\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 10/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0241 - normalizedMAE: 0.0241\n",
      "Epoch 00010: val_loss did not improve from 0.01703\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0241 - normalizedMAE: 0.0241 - val_loss: 0.0176 - val_normalizedMAE: 0.0176 - lr: 0.0010\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 11/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0237 - normalizedMAE: 0.0237\n",
      "Epoch 00011: val_loss improved from 0.01703 to 0.01608, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_1.h5\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0237 - normalizedMAE: 0.0237 - val_loss: 0.0161 - val_normalizedMAE: 0.0161 - lr: 0.0010\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 12/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0221 - normalizedMAE: 0.0221\n",
      "Epoch 00012: val_loss did not improve from 0.01608\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0221 - normalizedMAE: 0.0221 - val_loss: 0.0163 - val_normalizedMAE: 0.0163 - lr: 0.0010\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 13/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0208 - normalizedMAE: 0.0208\n",
      "Epoch 00013: val_loss improved from 0.01608 to 0.01467, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_1.h5\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0208 - normalizedMAE: 0.0208 - val_loss: 0.0147 - val_normalizedMAE: 0.0147 - lr: 0.0010\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 14/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.0207 - normalizedMAE: 0.0207\n",
      "Epoch 00014: val_loss did not improve from 0.01467\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0207 - normalizedMAE: 0.0207 - val_loss: 0.0217 - val_normalizedMAE: 0.0217 - lr: 0.0010\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 15/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0193 - normalizedMAE: 0.0193\n",
      "Epoch 00015: val_loss did not improve from 0.01467\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0193 - normalizedMAE: 0.0193 - val_loss: 0.0151 - val_normalizedMAE: 0.0151 - lr: 0.0010\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 16/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0180 - normalizedMAE: 0.0180\n",
      "Epoch 00016: val_loss did not improve from 0.01467\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0180 - normalizedMAE: 0.0180 - val_loss: 0.0169 - val_normalizedMAE: 0.0169 - lr: 0.0010\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 17/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.0177 - normalizedMAE: 0.0177\n",
      "Epoch 00017: val_loss did not improve from 0.01467\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0177 - normalizedMAE: 0.0177 - val_loss: 0.0190 - val_normalizedMAE: 0.0190 - lr: 0.0010\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 18/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0172 - normalizedMAE: 0.0172\n",
      "Epoch 00018: val_loss did not improve from 0.01467\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0172 - normalizedMAE: 0.0172 - val_loss: 0.0254 - val_normalizedMAE: 0.0254 - lr: 0.0010\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 19/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0161 - normalizedMAE: 0.0161\n",
      "Epoch 00019: val_loss improved from 0.01467 to 0.01385, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_1.h5\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0161 - normalizedMAE: 0.0161 - val_loss: 0.0139 - val_normalizedMAE: 0.0139 - lr: 0.0010\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 20/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0160 - normalizedMAE: 0.0160\n",
      "Epoch 00020: val_loss improved from 0.01385 to 0.01384, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_1.h5\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0160 - normalizedMAE: 0.0160 - val_loss: 0.0138 - val_normalizedMAE: 0.0138 - lr: 0.0010\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 21/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.0155 - normalizedMAE: 0.0155\n",
      "Epoch 00021: val_loss did not improve from 0.01384\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0155 - normalizedMAE: 0.0155 - val_loss: 0.0340 - val_normalizedMAE: 0.0340 - lr: 0.0010\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 22/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0169 - normalizedMAE: 0.0169\n",
      "Epoch 00022: val_loss did not improve from 0.01384\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0169 - normalizedMAE: 0.0169 - val_loss: 0.0152 - val_normalizedMAE: 0.0152 - lr: 0.0010\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 23/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0152 - normalizedMAE: 0.0152\n",
      "Epoch 00023: val_loss did not improve from 0.01384\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0152 - normalizedMAE: 0.0152 - val_loss: 0.0160 - val_normalizedMAE: 0.0160 - lr: 0.0010\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 24/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0150 - normalizedMAE: 0.0150\n",
      "Epoch 00024: val_loss improved from 0.01384 to 0.01189, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_1.h5\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0150 - normalizedMAE: 0.0150 - val_loss: 0.0119 - val_normalizedMAE: 0.0119 - lr: 0.0010\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 25/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0142 - normalizedMAE: 0.0142\n",
      "Epoch 00025: val_loss did not improve from 0.01189\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0142 - normalizedMAE: 0.0142 - val_loss: 0.0123 - val_normalizedMAE: 0.0123 - lr: 0.0010\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 26/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0142 - normalizedMAE: 0.0142\n",
      "Epoch 00026: val_loss did not improve from 0.01189\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0142 - normalizedMAE: 0.0142 - val_loss: 0.0210 - val_normalizedMAE: 0.0210 - lr: 0.0010\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 27/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0147 - normalizedMAE: 0.0147\n",
      "Epoch 00027: val_loss improved from 0.01189 to 0.01129, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_1.h5\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0147 - normalizedMAE: 0.0147 - val_loss: 0.0113 - val_normalizedMAE: 0.0113 - lr: 0.0010\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 28/100\n",
      "761/762 [============================>.] - ETA: 0s - loss: 0.0144 - normalizedMAE: 0.0144\n",
      "Epoch 00028: val_loss did not improve from 0.01129\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0144 - normalizedMAE: 0.0144 - val_loss: 0.0284 - val_normalizedMAE: 0.0284 - lr: 0.0010\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 29/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0137 - normalizedMAE: 0.0137\n",
      "Epoch 00029: val_loss did not improve from 0.01129\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0137 - normalizedMAE: 0.0137 - val_loss: 0.0158 - val_normalizedMAE: 0.0158 - lr: 0.0010\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 30/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0131 - normalizedMAE: 0.0131\n",
      "Epoch 00030: val_loss did not improve from 0.01129\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0131 - normalizedMAE: 0.0131 - val_loss: 0.0163 - val_normalizedMAE: 0.0163 - lr: 0.0010\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 31/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0135 - normalizedMAE: 0.0135\n",
      "Epoch 00031: val_loss improved from 0.01129 to 0.01039, saving model to /home/murillo/Dropbox/MNP/LIDIAgithub/RegressionNMAE/Trained_models/VGGEqNMAEBS32/RegressionVGG_1.h5\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0135 - normalizedMAE: 0.0135 - val_loss: 0.0104 - val_normalizedMAE: 0.0104 - lr: 0.0010\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 32/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0132 - normalizedMAE: 0.0132\n",
      "Epoch 00032: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0132 - normalizedMAE: 0.0132 - val_loss: 0.0226 - val_normalizedMAE: 0.0226 - lr: 0.0010\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 33/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0130 - normalizedMAE: 0.0130\n",
      "Epoch 00033: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0130 - normalizedMAE: 0.0130 - val_loss: 0.0159 - val_normalizedMAE: 0.0159 - lr: 0.0010\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 34/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0127 - normalizedMAE: 0.0127\n",
      "Epoch 00034: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0127 - normalizedMAE: 0.0127 - val_loss: 0.0254 - val_normalizedMAE: 0.0254 - lr: 0.0010\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 35/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0136 - normalizedMAE: 0.0136\n",
      "Epoch 00035: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0136 - normalizedMAE: 0.0136 - val_loss: 0.0123 - val_normalizedMAE: 0.0123 - lr: 0.0010\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 36/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0119 - normalizedMAE: 0.0119\n",
      "Epoch 00036: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0119 - normalizedMAE: 0.0119 - val_loss: 0.0111 - val_normalizedMAE: 0.0111 - lr: 0.0010\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 37/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0116 - normalizedMAE: 0.0116\n",
      "Epoch 00037: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0116 - normalizedMAE: 0.0116 - val_loss: 0.0146 - val_normalizedMAE: 0.0146 - lr: 0.0010\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 38/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0128 - normalizedMAE: 0.0128\n",
      "Epoch 00038: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0128 - normalizedMAE: 0.0128 - val_loss: 0.0177 - val_normalizedMAE: 0.0177 - lr: 0.0010\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 39/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0111 - normalizedMAE: 0.0111\n",
      "Epoch 00039: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0111 - normalizedMAE: 0.0111 - val_loss: 0.0123 - val_normalizedMAE: 0.0123 - lr: 0.0010\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 40/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0125 - normalizedMAE: 0.0125\n",
      "Epoch 00040: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 100ms/step - loss: 0.0125 - normalizedMAE: 0.0125 - val_loss: 0.0176 - val_normalizedMAE: 0.0176 - lr: 0.0010\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 41/100\n",
      "762/762 [==============================] - ETA: 0s - loss: 0.0130 - normalizedMAE: 0.0130\n",
      "Epoch 00041: val_loss did not improve from 0.01039\n",
      "762/762 [==============================] - 76s 99ms/step - loss: 0.0130 - normalizedMAE: 0.0130 - val_loss: 0.0161 - val_normalizedMAE: 0.0161 - lr: 0.0010\n",
      "Cleaning GPU memory...\n",
      "Execution time in Training:3133.612342034001\n"
     ]
    }
   ],
   "source": [
    "# Get the UNET model you want:\n",
    "input_img = Input((im_height, im_width, 1), name='img')\n",
    "\n",
    "lossVal=[]\n",
    "lossTrain=[]\n",
    "timeIterData=[]\n",
    "timeIterTrain=[]\n",
    "\n",
    "for k2 in range(numberOfTrainings):\n",
    "    print('\\n NUMBER OF TRAINING', k2+1)    \n",
    "    start = perf_counter()\n",
    "\n",
    "    \n",
    "    #JJMF 20230324 Randomness is an important issue here. In function create_datasetV10 we have seed as argument. If we use the same\n",
    "    #seed the same DA dataset will be generated. Otherwise the seed is ramdomized in the following lines. \n",
    "    \n",
    "    # Random seed\n",
    "    random.seed(datetime.now().microsecond)\n",
    "    np.random.seed(datetime.now().microsecond)\n",
    "    tf.random.set_seed(datetime.now().microsecond)\n",
    "\n",
    "    # CREATING DATASET\n",
    "    X_rot_train_full, aux_rot_train_full, dataAugmentationNumber, trainIdx, validIdx, testIdx = create_datasetV10(folderCrops, folderNPZ, idxImgs, \n",
    "                                            indexCanvas, labelsPerImage, trainImages, validImages, testImages, giro=False, seed=seedCreateDatasetV9+k2)\n",
    "\n",
    "    #JJMF 20230324 labels are annotation in an image, indicating where crossing points are. We next compute the density of \n",
    "    #vertical threads from the annotated images\n",
    "    # Generate count ground truth\n",
    "    print('Generating counting labels... ')\n",
    "    Y_train_full = np.zeros((len(aux_rot_train_full),1))\n",
    "    for k1 in range(len(aux_rot_train_full)):\n",
    "        Y_train_full[k1],_,_,_ = spatialCountingNeighborsv3(aux_rot_train_full[k1],pS=100,pI=0)\n",
    "    del aux_rot_train_full\n",
    "\n",
    "    end = perf_counter()\n",
    "    execution_time = (end - start)\n",
    "    print('Execution time in generating samples:{}'.format(execution_time)) \n",
    "    timeIterData.append(execution_time)    \n",
    "    \n",
    "    #TRAINING\n",
    "    start = perf_counter()\n",
    "    \n",
    "    print('Instancing new model...')\n",
    "    model = eval(modelFunc)\n",
    "    model.compile(optimizer = Adam(learning_rate = LR), loss = normalizedMAE, metrics = normalizedMAE)\n",
    "    mc = ModelCheckpoint(folderSave+nameModel2Save+'_'+str(k2)+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "    print('Training...')\n",
    "    # Training...\n",
    "    results = model.fit(X_rot_train_full[trainIdx], Y_train_full[trainIdx], epochs=EPOCHS, batch_size = BatchSize, shuffle=True, \n",
    "       validation_data=(X_rot_train_full[validIdx], Y_train_full[validIdx]),callbacks=[es, mc, sch])\n",
    "\n",
    "    print('Cleaning GPU memory...')\n",
    "    # Cleaning GPU memory\n",
    "    K.clear_session()\n",
    "    for i in range(50):\n",
    "        gc.collect()  \n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    del model, X_rot_train_full, Y_train_full\n",
    "    \n",
    "    #JJMF 20230324 We save history results\n",
    "    nameResults=folderSave+nameModel2Save+'Hist_'+str(k2)+'.npy'\n",
    "    np.save(nameResults,results.history)\n",
    "    #historyLoaded=np.load('nameResults',allow_pickle='TRUE').item()\n",
    "    \n",
    "    minVal= min(results.history.get('val_loss'))\n",
    "    idx = results.history.get('val_loss').index(minVal)\n",
    "    minTrain=results.history.get('loss')[idx]\n",
    "    lossVal.append(minVal)\n",
    "    lossTrain.append(minTrain)   \n",
    "    \n",
    "    end = perf_counter()\n",
    "    execution_time = (end - start)\n",
    "    print('Execution time in Training:{}'.format(execution_time)) \n",
    "    timeIterTrain.append(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ecf40f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegressionVGG</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lossTrain</td>\n",
       "      <td>0.026747</td>\n",
       "      <td>0.013485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lossVal</td>\n",
       "      <td>0.015926</td>\n",
       "      <td>0.010387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>timeIterData</td>\n",
       "      <td>388.025008</td>\n",
       "      <td>393.679132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>timeIterTrain</td>\n",
       "      <td>1396.748307</td>\n",
       "      <td>3133.612342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RegressionVGG            0            1\n",
       "0      lossTrain     0.026747     0.013485\n",
       "1        lossVal     0.015926     0.010387\n",
       "2   timeIterData   388.025008   393.679132\n",
       "3  timeIterTrain  1396.748307  3133.612342"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#JJMF 20230324 We save the results of the train and validation loss and invested times\n",
    "import pandas as pd\n",
    "losses=np.c_[lossTrain, lossVal, timeIterData, timeIterTrain]\n",
    "iterations=[]\n",
    "for k2 in range(numberOfTrainings):\n",
    "    iterations.append(str(k2))\n",
    "df = pd.DataFrame(losses.T, columns = iterations) #['0','1','2','3','4','5','6','7','8','9'])\n",
    "\n",
    "lossNames = ['lossTrain', 'lossVal', 'timeIterData', 'timeIterTrain']\n",
    "df[nameModel2Save]=lossNames\n",
    "df.set_index(nameModel2Save,inplace=True)\n",
    "namePD=folderSave+nameModel2Save+'Losses'+'.csv'\n",
    "df.to_csv(namePD)\n",
    "df2=pd.read_csv(namePD)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d047128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Iteration:  1\n"
     ]
    }
   ],
   "source": [
    "#JJMF 20230324 The selected weights are those for the train with lowest validation loss\n",
    "bestTraining=np.argmin(losses[:,1])\n",
    "print('Best Iteration: ',bestTraining) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18f409d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#JJMF 20230324 Load saved results and plot them\n",
    "for k2 in range(numberOfTrainings):\n",
    "    \n",
    "    plt.figure\n",
    "    nameResults=folderSave+nameModel2Save+'Hist_'+str(k2)+'.npy'\n",
    "    historyLoaded=np.load(nameResults,allow_pickle='TRUE').item()\n",
    "    plt.plot(historyLoaded['loss'],label='loss')\n",
    "    plt.plot(historyLoaded['val_loss'],label='val_loss')\n",
    "   \n",
    "    plt.title('Iteration:'+str(k2))\n",
    "    plt.legend()\n",
    "    plt.ylim([0,.1])\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874b131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unet4threadsV5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
